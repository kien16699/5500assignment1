---
title: "assignment1"
author: "Dimensio Reducto"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Data Description

Inequality data has eleven variables, representing 52 different states of the U.S. on the average income inequality. This panel data set variables are the Gini index from 1918 to 2018, recorded every ten years for a total of eleven Gini index variables. In this report, the use of principle components analysis, cluster analysis, and multidimensional scaling will help investigate how inequality has evolved over time for different states in the U.S.

# Preliminary Analysis

```{r}
library(tidyverse)
library(MASS)
library(patchwork)
library(mice)
library(Amelia)
library(gghighlight)
```


```{r}
df <- read_csv("Inequality_data.csv")
```

```{r}
summary(df)
```

There are a few anomalies such as 2 NA values for all years from 1918 to 1958 with the exception of year 1948 having 20 NA values. Additionally the highest Gini index for the year 1968 is 1000, which is definite a mistake considering the range of Gini index value is between 0 and 1.

```{r}
df %>% pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot()
```

We can see from the box plot that the variance of gini index, signifies by the interquartile range, between states was higher for the first 2 periods, 1918 and 1928, and then gradually reduced, reaching lowest variance in 1968 but then started to slightly increase again over the next periods. We can see that quite a few periods there are outliers such as in the years of 1918, 1928, 1938, 1958, 1968, 1988 and 1998. However, only the year 1938 has an extreme value, which is when the outliers is 3*IQR greater than the 1st interquartile. We can see all the outliers below

```{r}
boxplot.stats(df$`1938`)$out -> outliers
df[which(df$`1938` %in% c(outliers)),]
```

The extreme value for the year 1938 is Delaware, with Gini index at 0.674. From the box plot we can also see that the largest Gini index across the whole data set comes from the year 1928, and from the table above, we can see that the point is also Delaware, with Gini at 0.747, this means that during the period 1928 to 1938, the state of Delaware had very wide income gap.

In terms of the distribution of gini index across the years, we can see that the data is quite normally distributed. We can see that the only period which is visibly positively skewed is the year 1998, where the mean is very close to the 1st quartile.

# Principle Component Analysis

### Handling of missing value
```{r}
# data prepare
df <- read.csv("Inequality_data.csv")
df$X1968[which(df$X1968 == 1000)] = NA

df_tidy <- df %>% 
  column_to_rownames('State')
```

As addressed by the previous analysis, there are a substantial number of missing values in the dataset. In this section, we have performed 3 different ways to handle those missing values. 

First, we have used the MICE (Multiple Imputation by Chained Equations) algorithm. This is a robust, informative method of dealing with missing data in the dataset. The procedure imputes missing data in a dataset through an iterative series of predictive models. In each iteration, each specified variable in the dataset is imputed using the other variables in the dataset. These iterations should be run until it appears that convergence has been met. 

```{r}
# mice: pmm
# the default assumption of pmm is missing at random
# missing at random means that the likelihood of missing a value depends on other variables, variables are correlated.
# using linear regression to predict missing, PCA also 

imputedData <- mice(df_tidy, m=5, maxit = 50, method = 'pmm', seed = 500)
# summary(imputedData)
completeData <- complete(imputedData,2)

# check the output of reassigning NA
mean(completeData$X1918)
mean(df_tidy$X1918, na.rm = T)


p_complete <- completeData %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() + 
  labs(x = "", title = "MICE dataset")

p_og <- df %>% 
  pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot()  +
  labs(title = "Original dataset")

p_complete/p_og
```

After filling in the missing values with mice algorithm, we can see that the mean of the dataset actually hasn't changed much, this is because the procedure we have used is called predictive mean matching (PMM). However, the issue with outliers has clearly improved. For the year 1918 and 1958, there were 2 outliers in the original dataset, and now are reduced to 1 after filling in the missing values. 

The second method we have tried is to use the package Amelia which impute the missing values probabilistically. This method conducts multiple imputation. It assumes the data is multivariate normal distribution, and it imputes m values for each missing values creating m completed datasets, then analyze each of m completed datasets seperately and finally combine the m results by taking the average and adjust the standard error. This procedure is similar to use bootstrap to simulate by independent variable and the algorithm is called Expectation-maximization with bootstraping. This method produces unbiased estimates. However, the limitation is that the values are imputed with uncertainty. In this case, same imputed values are negative which does not comply with our scenario where gini index is between 0 to 1. 

```{r}
# Amelia
# assumption: Multivariate Normal Distribution
# use bootstrap to simulate by independent variable

set.seed(444) # not work
amelia_fit <- amelia(df_tidy, m=5, parallel = "multicore", startvals = 1)
df_amelia <- amelia_fit$imputations[[1]]

p_amelia <- df_amelia %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() + 
  labs(x = "", title = "Amelia dataset")

p_complete/p_og
```

After filling in the missing values with amelia package, we can see that the issue with outliers is also improved. Same as mice method, the outliers in year 1918 are reduced to 1. 

The third method is to use the mean or median or nearby values to fill in the missing values. The advantage of this method is easy computing, but of course it has a clear limitation which could lead to biased estimates, and it does not account for uncertainty of the imputed values.

```{r}
# mean
df_mean <- data.frame(apply(df_tidy, 2, function(x){
    x[is.na(x)] = mean(x, na.rm = T); x}))
```

In summary, all 3 methods can to some extend solve the problem of missing values. While the mean method could create biased estimates, the Amelia method also has uncertainty to the imputed values. Hence, we use the mice algorithm to handle the missing value and will use the filled in datasets for the following analysis.

### PCA

Principal components analysis finds a small number of linear combinations of the original variables that explain a large proportion of overall variation in the data. Since the variables in the dataset under investigation are measured in the same units, we don't need to standardise the data. 

```{r}
# completeData for MICE, df_amelia for Amelia, df_mean for mean
completeData %>% 
  prcomp() -> pcaout
summary(pcaout) 
```

From the summary report, we can see that there are 11 principal components are obtained. Each of these explains a percentage of the total variation in the dataset. That is to say: PC1 explains 58% of the total variance, while PC2 explains 15% of the total variance, as just PC1 and PC2 can explain 73% of the total variance.


```{r}
# loadings
as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC1)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC1), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC1") 

as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC2)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC2), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC2") 
```

By visualizing the loading plots, we could identify the variable importance to each principal components. While the red lines represents the confidence interval, variables that exceed the red lines are significantly different from zero, which means having more contribution to the principal components. We can see that year 1918,1928 and 1938 are more influential to PC1, while year 1988,1998,2008 and 2018 are more influential to PC2.

A scree plot, on the other hand, is a diagnostic tool to check whether PCA works well on the data. PC1 captures the most variation, PC2 â€” the second most, and so on, and each of them contributes some information of the data. Here we can see that PC1 and PC2 have captured most of the variation. 

```{r}
# number of PCs
screeplot(pcaout,type = 'l')  

# correlation
biplot(pcaout,scale=0, cex=0.7)

# distance
biplot(pcaout, cex=0.7)
```

By selecting two principal components we are able to visualize the data using a biplot. The first biplot shows the correlation among variables. We can see year 1928, 1918 and 1938 are positively correlated. One possible explanation could be due to the World War 1 and 2 during that period of time. The rest of the variables are positively correlated. The second biplot implies the distance. While 1928 contributes the most to the PC1, 2018 contributes the most to the PC2. Also, the closer distance between states, the more similar they are. While most observations are clustered in the top right side of the plot, Delaware, New York and Florida are obviously different from the others.


# Multidimensional Scaling

```{r}
dd <- completeData %>%
  dist(method = "euclidean")

completeData %>%
  rownames_to_column() %>%
  pull(rowname) -> attributes(dd)$Labels

cmds <- cmdscale(dd, eig = T)

cmds_point <- cmds$points

cmds_point %>%
  as_tibble(rownames = "State") %>%
  ggplot(aes(x = V1, y = V2, label = State)) +
  geom_text(size = 3) +
  gghighlight(State %in% c("Delaware", "New York", "Florida"), unhighlighted_colour = "dark grey")
```

Using the classical Multidimensional Scaling, it finds a low dimensional representation of the dataset by minimise strain. From the plot above, we can see the potential 3 outliers of the dataset; Delaware, New York, and Florida. This result is the same as what we have observed in the PCA, also the states the are closer together the closer they are, as this plot represent the similarity between states.

```{r}
cmds$GOF
```

The *GOF* tell us how good the fit are, the number are the same for both measures. It is because we use Euclidean distances this will always give positive eigenvalues, and since first measure use $|\lambda_i|$ and second measure use $max(0, \lambda_i)$. Therefore when using the Euclidean distance it will give the same results. With the goodness of fit measures are quite high, we can believe that the solution is an accurate representation.


# Cluster Analysis


```{r}
df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(`1968` != 1000) %>% 
  column_to_rownames(var = "State") %>% 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
    dist(method = 'euclidean')->d #compute distance matrix 
hclust(d,method='ward.D2')->hcl
hcl %>%  plot(cex = 0.6)
hcl %>% rect.hclust(k = 6)
```

From the dendogram above we can see that using the Ward clustering method, we can cut the tree into either 4 or 6 clusters for stable solutions. Because 3 and 5 clusters are not stable and can change over a short range of tolerance, while 1 and 2 clusters not a good choice as there are room to form better defined clusters.

What's more interesting to see is that we can identify there is a small cluster with only Delaware and Alaska. As we have identified Delaware as the extreme outlier before in the boxplot, we can expect this cluster to contains only outliers. If we go back to the box plot and identify the second most outlying point in Gini index, which is in the year 1988, we can see that the state is indeed Alaska as below

```{r}
boxplot.stats(df$`1988`)$out -> outliers88
df[which(df$`1988` %in% c(outliers88)),]
```

Using other method of clustering, such as average or centroid linkage

```{r}
df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(`1968` != 1000) %>% 
  column_to_rownames(var = "State") %>% 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
    dist(method = 'manhattan')->d #compute distance matrix 
hclust(d,method='ward.D2')->hclman
hclman %>%  plot(cex = 0.6)
```

If we use manhattan distance instead to cluster the states, the stable solution would be 4 or 7 clusters instead of 4 or 6 as in the case of euclidean distance. More interesting is if the cluster was 7, we can identify another small cluster consisting of only the states of New York and Florida. 

To see if the members of the state are different or not as compared to when euclidean distance is used, we can assume the cluster is 4 and use Rand index:

```{r}
library(mclust) 
hclman %>% cutree(k = 4) -> memb_man
hcl %>% cutree(k = 4) -> memb_eu
adjustedRandIndex(memb_eu,memb_man)
```

The accordance rate is 0.64 so the 2 distance somewhat agrees on the clustering of the states

