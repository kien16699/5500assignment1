---
title: "Assignment1"
author: "Dimensio Reducto"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Data Description

Inequality data has eleven variables, representing the average income inequality in the U.S. for 51 different states and an aggregate of these states. This panel data set variables are the Gini index from 1918 to 2018, recorded every ten years for a total of eleven Gini index variables. In this report, the use of principle components analysis, cluster analysis, and multidimensional scaling will help investigate how inequality has evolved over time for different states in the U.S.

# Preliminary Analysis

```{r}
library(tidyverse)
library(MASS)
library(naniar)
library(mice)
library(Amelia)
library(gghighlight)
```


```{r}
df <- read_csv("Inequality_data.csv")
```

### Overview of the raw data

```{r}
gg_miss_upset(df)
```

There are a few anomalies such as 2 NA values for all years from 1918 to 1958 with the exception of year 1948 having 20 NA values.

```{r}
summary(df)
```

Additionally the highest Gini index for the year 1968 is 1000, which is definite a mistake considering the range of Gini index value is between 0 and 1.

```{r}
df %>% pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot()
```

We can see from the box plot that the variance of gini index, signifies by the interquartile range, between states was higher for the first 2 periods, 1918 and 1928, and then gradually reduced, reaching lowest variance in 1968 but then started to slightly increase again over the next periods. We can see that quite a few periods there are outliers such as in the years of 1918, 1928, 1938, 1958, 1968, 1988 and 1998. However, only the year 1938 has an extreme value, which is when the outliers is 3*IQR greater than the 1st interquartile. We can see all the outliers below.

```{r}
boxplot.stats(df$`1938`)$out -> outliers
df[which(df$`1938` %in% c(outliers)),]
```

The extreme value for the year 1938 is Delaware, with Gini index at 0.674. From the box plot we can also see that the largest Gini index across the whole data set comes from the year 1928, and from the table above, we can see that the point is also Delaware, with Gini at 0.747, this means that during the period 1928 to 1938, the state of Delaware had very wide income gap.

In terms of the distribution of gini index across the years, we can see that the data is quite normally distributed. We can see that the only period which is visibly positively skewed is the year 1998, where the mean is very close to the 1st quantile.

### Reassignment of missing values

```{r}
# data prepare
df$`1968`[which(df$`1968` == 1000)] = NA

df_tidy <- df %>% 
  mutate(State = recode(State, "United States" = "US",
                        "District of Columbia" = "DC"))
df_tidy$State <- ifelse(nchar(df_tidy$State) > 2,
                        state.abb[match(df_tidy$State,state.name)],
                        df_tidy$State)
df_tidy <- column_to_rownames(df_tidy, 'State') 
colnames(df_tidy) <- paste("X", colnames(df_tidy), sep = ".")
```

The reason that the aggregate item United States is not removed is that we want to keep this as an observation for reassigning NA. At the same time, although the reason for missing data before 1958 like Alaska and Hawaii is that two states were only admitted into the union in 1959, in order to ensure the integrity of the data, we chose to predict and fill the NA by regression fitting rather than dropping them directly. Additionally, the highest gini index 1000 for the year 1968 is changed to NA firstly, then will perform 3 different ways to handle all missing values. 

```{r, results='hide'}
# pmm of mice
imputedData <- mice(df_tidy, m=5, maxit = 50, method = 'pmm', seed = 500)
completeData <- complete(imputedData,2)
```

First, we have used the MICE (Multiple Imputation by Chained Equations) algorithm. This is a robust, informative method of dealing with missing data in the dataset. The procedure imputes missing data in a dataset through an iterative series of predictive models. In each iteration, each specified variable in the dataset is imputed using the other variables in the dataset. These iterations should be run until it appears that convergence has been met. 

```{r}
tibble(raw_1918 = mean(df_tidy$X.1918, na.rm = T),
       mice_1918 = mean(completeData$X.1918)) 
```

After filling in the missing values with mice algorithm, we can see that the mean of the dataset actually hasn't changed much, this is because the procedure we have used is called predictive mean matching (PMM). 

```{r}
p_complete <- completeData %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() +
  scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
  theme(axis.text.x = element_text(size = 7)) +
  labs(title = "MICE dataset")

p_og <- df %>% 
  pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(size = 7)) +
  labs(title = "Original dataset")

gridExtra::grid.arrange(p_complete,p_og, nrow=1)
```

However, the issue with outliers has clearly improved. For the year 1918 and 1958, there were 2 outliers in the original dataset, and now are reduced to 1 after filling in the missing values. 

```{r, results='hide'}
# Amelia
set.seed(444) 
amelia_fit <- amelia(df_tidy, m=5, parallel = "multicore", startvals = 1)
df_amelia <- amelia_fit$imputations[[1]]
```

The second method we have tried is to use the package Amelia which impute the missing values probabilistically. This method conducts multiple imputation. It assumes the data is multivariate normal distribution, and it imputes m values for each missing values creating m completed datasets, then analyze each of m completed datasets seperately and finally combine the m results by taking the average and adjust the standard error. This procedure is similar to use bootstrap to simulate by independent variable and the algorithm is called Expectation-maximization with bootstrapping. This method produces unbiased estimates. However, the limitation is that the values are imputed with uncertainty. In this case, same imputed values are negative which does not comply with our scenario where gini index is between 0 to 1. 

```{r, results='hide'}
p_amelia <- df_amelia %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() + 
  scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
  theme(axis.text.x = element_text(size = 7)) +
  labs(x = "", title = "Amelia dataset")

gridExtra::grid.arrange(p_amelia,p_og, nrow=1)
```

After filling in the missing values with amelia package, we can see that the outliers in the year 1918 are reduced to 1, but from 1928 to 1948 there were more new outliers.

```{r}
# mean
df_mean <- data.frame(apply(df_tidy, 2, function(x){
    x[is.na(x)] = mean(x, na.rm = T); x}))
```

The third method is to use the mean or median or nearby values to fill in the missing values. The advantage of this method is easy computing, but of course it has a clear limitation which could lead to biased estimates, and it does not account for uncertainty of the imputed values.

In summary, all 3 methods can to some extend solve the problem of missing values. While the mean method could create biased estimates, the Amelia method also has uncertainty to the imputed values. Hence, we use the mice algorithm to handle the missing value and will use the filled in datasets for the following analysis.

# Principle Component Analysis

Principal components analysis finds a small number of linear combinations of the original variables that explain a large proportion of overall variation in the data. Since the variables in the dataset under investigation are measured in the same units, we don't need to standardise the data. 

```{r}
completeData %>% 
  prcomp() -> pcaout
summary(pcaout) 
```

From the summary report, we can see that there are 11 principal components are obtained. Each of these explains a percentage of the total variation in the dataset. That is to say: PC1 explains 58% of the total variance, while PC2 explains 15% of the total variance, as just PC1 and PC2 can explain 73% of the total variance.


```{r}
# loadings
as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC1)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC1), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC1") 

as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC2)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC2), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC2") 
```

By visualizing the loading plots, we could identify the variable importance to each principal components. While the red lines represents the confidence interval, variables that exceed the red lines are significantly different from zero, which means having more contribution to the principal components. We can see that year 1918,1928 and 1938 are more influential to PC1, while year 1988,1998,2008 and 2018 are more influential to PC2.

```{r}
# number of PCs
screeplot(pcaout,type = 'l')
```

A scree plot, on the other hand, is a diagnostic tool to check whether PCA works well on the data. PC1 captures the most variation, PC2 — the second most, and so on, and each of them contributes some information of the data. Here we can see that PC1 and PC2 have captured most of the variation.

```{r, fig.width=8, fig.height=8}
# correlation
biplot(pcaout,scale=0, cex=0.7)
```

By selecting two principal components we are able to visualize the data using a biplot. The first biplot shows the correlation among variables. We can see year 1928, 1918 and 1938 are positively correlated. One possible explanation could be due to the World War 1 and 2 during that period of time. The rest of the variables are positively correlated.

```{r, fig.width=8, fig.height=8}
# distance
biplot(pcaout, cex=0.7)
```

The second biplot implies the distance. While 1928 contributes the most to the PC1, 2018 contributes the most to the PC2. Also, the closer distance between states, the more similar they are. While most observations are clustered in the top right side of the plot, Delaware, New York and Florida are obviously different from the others.


# Multidimensional Scaling

```{r}
dd <- completeData %>%
  dist(method = "euclidean")

completeData %>%
  rownames_to_column() %>%
  pull(rowname) -> attributes(dd)$Labels

cmds <- cmdscale(dd, eig = T)

cmds_point <- cmds$points

cmds_point %>%
  as_tibble(rownames = "State") %>%
  ggplot(aes(x = V1, y = V2, label = State)) +
  geom_text(size = 3) +
  gghighlight(State %in% c("Delaware", "New York", "Florida"), unhighlighted_colour = "dark grey")
```

Using the classical Multidimensional Scaling, it finds a low dimensional representation of the dataset by minimizing strain. From the plot above, we can see the potential three outliers of the dataset; Delaware, New York, and Florida. This result is the same as what we have observed in the PCA. Also, the states that are closer together, the closer they are, as this plot represents the similarity between states.

```{r}
cmds$GOF
```

The *GOF* tells us how good the fit is. The numbers are the same for both measures. It is because we use Euclidean distances, which will always give positive eigenvalues, and since the first measure use $|\lambda_i|$ and the second measure uses $max(0, \lambda_i)$. Therefore when using the Euclidean distance, it will give the same results. With the goodness of fit measures being quite high, we can believe that the solution is an accurate representation.


# Cluster Analysis


```{r}
# df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
#   filter(`1968` != 1000) %>% 
#   column_to_rownames(var = "State") 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
completeData %>% 
    dist(method = 'euclidean')->d #compute distance matrix 
hclust(d,method='ward.D2')->hcl
hcl %>%  plot(cex = 0.6)
hcl %>% rect.hclust(k = 4)
```

From the dendogram above we can see that using the Ward clustering method, we can cut the tree into 4 clusters for stable solutions. Because 3 clusters is not stable and can change over a short range of tolerance, while 5 and above will not include the cluster with Delaware state. 1 and 2 clusters not a good choice as there are room to form better defined clusters.

What's more interesting to see is that, aside from Delaware, we can identify another less smaller outlier which is Alaska. If we go back to the box plot and identify the second most outlying point in Gini index, which is in the year 1988, we can see that the state is indeed Alaska as below

```{r}
boxplot.stats(df$`1988`)$out -> outliers88
df[which(df$`1988` %in% c(outliers88)),]
```

If we use manhattan distance instead to cluster the states, the stable solution would also be 4, similar to that as in the case of euclidean distance. More interestingly is we can identify New York and Florida which are previously within the same cluster, if we decided the number of cluster to be higher, and is a potential outlier as compared to the majority of other clusters. These only happens when Euclidean distance is used but using Manhattan distance in clustering, they are not within the same cluster anymore nor do they look like potential outliers

```{r}
# df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
#   filter(`1968` != 1000) %>% 
#   column_to_rownames(var = "State") %>% 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
completeData %>% 
    dist(method = 'manhattan')->dman #compute distance matrix 
hclust(dman,method='ward.D2')->hclman
hclman %>%  plot(cex = 0.6)
hclman %>% rect.hclust(k = 4)
```


To see if the members of the state are different or not as compared to when euclidean distance is used, we can assume the cluster is 4 and use Rand index:

```{r}
library(mclust) 
hclman %>% cutree(k = 4) -> memb_man
hcl %>% cutree(k = 4) -> memb_eu
adjustedRandIndex(memb_eu,memb_man)
```

It's interesting to see how when different distance is used, the number of clusters are still the same but the members of each cluster are all difference, as the adjusted Rand Index is only 0.2

# Limitation

- **Data:** the obvious limitation of this data is the null values, although using many different ways to compare and finally choosing the Predictive Mean Matching from the MICE package to reassign, it is also small biased from the actual situation.
- **PCA:** one limitation is the low interpretability of PCA, although the first two PCs have explained 73% of the total variance, it is difficult to generalize what the new index specifically means. On the other hand, PCA is sensitive to the scale of the features and not robust against outliers, so we tried a variety of NA reassignment methods to minimize the bias from the data.


