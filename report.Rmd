---
title: "assignment1"
author: "Dimensio Reducto"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Data Description

Inequality data has eleven variables, representing 52 different states of the U.S. on the average income inequality. This panel data set variables are the Gini index from 1918 to 2018, recorded every ten years for a total of eleven Gini index variables. In this report, the use of principle components analysis, cluster analysis, and multidimensional scaling will help investigate how inequality has evolved over time for different states in the U.S.

# Preliminary Analysis

```{r}
library(tidyverse)
library(MASS)
```


```{r}
df <- read_csv("Inequality_data.csv")
```

```{r}
summary(df)
```

There are a few anomalies such as 2 NA values for all years from 1918 to 1958 with the exception of year 1948 having 20 NA values. Additionally the highest Gini index for the year 1968 is 1000, which is definite a mistake considering the range of Gini index value is between 0 and 1.

```{r}
df %>% pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() 
```

We can see from the box plot that the variance of gini index, signifies by the interquartile range, between states was higher for the first 2 periods, 1918 and 1928, and then gradually reduced, reaching lowest variance in 1968 but then started to slightly increase again over the next periods. We can see that quite a few periods there are outliers such as in the years of 1918, 1928, 1938, 1958, 1968, 1988 and 1998. However, only the year 1938 has an extreme value, which is when the outliers is 3*IQR greater than the 1st interquartile. We can see all the outliers below

```{r}
boxplot.stats(df$`1938`)$out -> outliers
df[which(df$`1938` %in% c(outliers)),]
```

The extreme value for the year 1938 is Delaware, with Gini index at 0.674. From the box plot we can also see that the largest Gini index across the whole data set comes from the year 1928, and from the table above, we can see that the point is also Delaware, with Gini at 0.747, this means that during the period 1928 to 1938, the state of Delaware had very wide income gap.

In terms of the distribution of gini index across the years, we can see that the data is quite normally distributed. We can see that the only period which is visibly positively skewed is the year 1998, where the mean is very close to the 1st quartile.

# Principle Component Analysis

### Handling of missing value
```{r}
# data prepare
df <- read.csv("Inequality_data.csv")
df$X1968[which(df$X1968 == 1000)] = NA

df_tidy <- df %>% 
  column_to_rownames('State')
```

```{r}
# mice: pmm
# the default assumption of pmm is missing at random
# missing at random means that the likelihood of missing a value depends on other variables, variables are correlated.
# using linear regression to predict missing, PCA also 

imputedData <- mice(df_tidy, m=5, maxit = 50, method = 'pmm', seed = 500)
# summary(imputedData)
completeData <- complete(imputedData,2)

# check the output of reassigning NA
mean(completeData$X1918)
mean(df_tidy$X1918, na.rm = T)

completeData %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() 

df %>% 
  pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() 
```

```{r}
# Amelia
# assumption: Multivariate Normal Distribution
# use bootstrap to simulate by independent variable

set.seed(444) # not work
amelia_fit <- amelia(df_tidy, m=5, parallel = "multicore", startvals = 1)
df_amelia <- amelia_fit$imputations[[1]]
```

```{r}
# mean
df_mean <- data.frame(apply(df_tidy, 2, function(x){
    x[is.na(x)] = mean(x, na.rm = T); x}))
```

### PCA

```{r}
# completeData for MICE, df_amelia for Amelia, df_mean for mean
completeData %>% 
  prcomp() -> pcaout

summary(pcaout) 
```

```{r}
# loadings
as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC1)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC1), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC1") 

as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC2)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC2), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC2") 
```

```{r}
# number of PCs
screeplot(pcaout,type = 'l')  

# correlation
biplot(pcaout,scale=0, cex=0.7)

# distance
biplot(pcaout, cex=0.7)
```

# Multidimensional Scaling

```{r}
library(gghighlight)
dd <- df %>%
  dist

df %>%
  pull(State) -> attributes(dd)$Labels

cmds <- cmdscale(dd)

cmds %>%
  as_tibble(rownames = "State") %>%
  ggplot(aes(x = V1, y = V2, label = State)) +
  geom_text() +
  gghighlight(State == "Alaska")
```

```{r}
dd <- df_amelia %>%
  dist

df_amelia %>%
  pull(State) -> attributes(dd)$Labels

cmds <- cmdscale(dd)

cmds %>%
  as_tibble(rownames = "State") %>%
  ggplot(aes(x = V1, y = V2, label = State)) +
  geom_text()
```



# Cluster Analysis


```{r}
df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(`1968` != 1000) %>% 
  column_to_rownames(var = "State") %>% 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
    dist(method = 'euclidean')->d #compute distance matrix 
hclust(d,method='ward.D2')->hcl
hcl %>%  plot(cex = 0.6)
hcl %>% rect.hclust(k = 6)
```

From the dendogram above we can see that using the Ward clustering method, we can cut the tree into either 4 or 6 clusters for stable solutions. Because 3 and 5 clusters are not stable and can change over a short range of tolerance, while 1 and 2 clusters not a good choice as there are room to form better defined clusters.

What's more interesting to see is that we can identify there is a small cluster with only Delaware and Alaska. As we have identified Delaware as the extreme outlier before in the boxplot, we can expect this cluster to contains only outliers. If we go back to the box plot and identify the second most outlying point in Gini index, which is in the year 1988, we can see that the state is indeed Alaska as below

```{r}
boxplot.stats(df$`1988`)$out -> outliers88
df[which(df$`1988` %in% c(outliers88)),]
```

Using other method of clustering, such as average or centroid linkage

```{r}
df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(`1968` != 1000) %>% 
  column_to_rownames(var = "State") %>% 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
    dist(method = 'manhattan')->d #compute distance matrix 
hclust(d,method='ward.D2')->hclman
hclman %>%  plot(cex = 0.6)
```

If we use manhattan distance instead to cluster the states, the stable solution would be 4 or 7 clusters instead of 4 or 6 as in the case of euclidean distance. More interesting is if the cluster was 7, we can identify another small cluster consisting of only the states of New York and Florida. 

To see if the members of the state are different or not as compared to when euclidean distance is used, we can assume the cluster is 4 and use Rand index:

```{r}
library(mclust) 
hclman %>% cutree(k = 4) -> memb_man
hcl %>% cutree(k = 4) -> memb_eu
adjustedRandIndex(memb_eu,memb_man)
```

The accordance rate is 0.64 so the 2 distance somewhat agrees on the clustering of the states

