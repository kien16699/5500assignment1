---
title: "Assignment1"
author: "Dimensio Reducto"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    df_print: kable
    fig_caption: yes
header-includes:
  \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center",fig.width = 6, fig.height = 3.5)
```

# Data Description

Inequality data has eleven variables, representing the average income inequality in the U.S. for 51 different states and an aggregate of these states. This panel data set variables are the Gini index from 1918 to 2018, recorded every ten years for a total of eleven Gini index variables. In this report, the use of principle components analysis, cluster analysis, and multidimensional scaling will help investigate how inequality has evolved over time for different states in the U.S.

# Preliminary Analysis

```{r}
library(tidyverse)
library(MASS)
library(naniar)
library(mice)
library(Amelia)
library(gghighlight)
library(kableExtra)
```


```{r}
df <- read_csv("Inequality_data.csv")
```

### Overview of the raw data

```{r, fig.cap = "Bar plots showing the missing values", fig.pos = "H"}
gg_miss_upset(df)
```

There are a few anomalies such as 2 NA values for all years from 1918 to 1958 with the exception of year 1948 having 20 NA values.

\clearpage

```{r}
summary(df) %>%
  kable(caption = "Summary for each variables of the dataset") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```

Additionally the highest Gini index for the year 1968 is 1000, which is definite a mistake considering the range of Gini index value is between 0 and 1.

```{r, fig.cap = "The box plot of original variables", fig.pos = "H"}
df %>% pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot()
```

We can see from the box plot that the variance of gini index, signifies by the interquartile range, between states was higher for the first 2 periods, 1918 and 1928, and then gradually reduced, reaching lowest variance in 1968 but then started to slightly increase again over the next periods. We can see that quite a few periods there are outliers such as in the years of 1918, 1928, 1938, 1958, 1968, 1988 and 1998. However, only the year 1938 has an extreme value, which is when the outliers is 3*IQR greater than the 1st interquartile. We can see all the outliers below.

```{r}
boxplot.stats(df$`1938`)$out -> outliers
df[which(df$`1938` %in% c(outliers)),] %>%
  kable(caption = "The summary table of the potential outlier states") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```

The extreme value for the year 1938 is Delaware, with Gini index at 0.674. From the box plot we can also see that the largest Gini index across the whole data set comes from the year 1928, and from the table above, we can see that the point is also Delaware, with Gini at 0.747, this means that during the period 1928 to 1938, the state of Delaware had very wide income gap.

\clearpage

In terms of the distribution of gini index across the years, we can see that the data is quite normally distributed. We can see that the only period which is visibly positively skewed is the year 1998, where the mean is very close to the 1st quantile.

### Reassignment of missing values

```{r}
# data prepare
df$`1968`[which(df$`1968` == 1000)] = NA

df_tidy <- df %>% 
  mutate(State = recode(State, "United States" = "US",
                        "District of Columbia" = "DC"))
df_tidy$State <- ifelse(nchar(df_tidy$State) > 2,
                        state.abb[match(df_tidy$State,state.name)],
                        df_tidy$State)
df_tidy <- column_to_rownames(df_tidy, 'State') 
colnames(df_tidy) <- paste("X", colnames(df_tidy), sep = ".")
```

The reason that the aggregate item United States is not removed is that we want to keep this as an observation for reassigning NA. At the same time, although the reason for missing data before 1958 like Alaska and Hawaii is that two states were only admitted into the union in 1959, in order to ensure the integrity of the data, we chose to predict and fill the NA by regression fitting rather than dropping them directly. Additionally, the highest gini index 1000 for the year 1968 is changed to NA firstly, then will perform 3 different ways to handle all missing values. 

```{r, results='hide'}
# pmm of mice
imputedData <- mice(df_tidy, m=5, maxit = 50, method = 'pmm', seed = 500)
completeData <- complete(imputedData,2)
```

First, we have used the MICE (Multiple Imputation by Chained Equations) algorithm. This is a robust, informative method of dealing with missing data. The procedure imputes missing data through an iterative series of predictive models. In each iteration, each specified variable is imputed using the other variables in the dataset. 

```{r}
tibble(raw_1918 = mean(df_tidy$X.1918, na.rm = T),
       mice_1918 = mean(completeData$X.1918)) %>%
  kable(caption = "The table showing the mean after filling the missing values") %>%
  kable_styling(latex_options = "hold_position")
```

After filling in the missing values with mice algorithm, we can see that the mean of the dataset actually hasn't changed much, this is because the procedure we have used is called predictive mean matching (PMM). 

```{r, fig.cap = "The summary comparing the new MICE dataset and the Original dataset", fig.pos = "H"}
p_complete <- completeData %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() +
  scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
  theme(axis.text.x = element_text(size = 7)) +
  labs(title = "MICE dataset")

p_og <- df %>% 
  pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
  filter(gini != 1000) %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(size = 7)) +
  labs(title = "Original dataset")

gridExtra::grid.arrange(p_complete,p_og, nrow=1)
```

However, the issue with outliers has clearly improved. For the year 1918 and 1958, there were 2 outliers in the original dataset, and now are reduced to 1 after using mice. 

```{r, results='hide'}
# Amelia
set.seed(444) 
amelia_fit <- amelia(df_tidy, m=5, parallel = "multicore", startvals = 1)
df_amelia <- amelia_fit$imputations[[1]]
```

The second method we have tried is to use the package Amelia which impute the missing values probabilistically. This method conducts multiple imputation. It assumes the data is multivariate normal distribution, and it imputes m values for each missing values creating m completed datasets, then analyze each of m completed datasets separately and finally combine the m results by taking the average and adjust the standard error. This procedure is similar to use bootstrap to simulate by independent variable and the algorithm is called Expectation-maximization with bootstrapping. This method produces unbiased estimates. However, the limitation is that the values are imputed with uncertainty. In this case, same imputed values are negative which does not comply with our scenario where gini index is between 0 to 1. 

```{r, fig.cap = "The summary comparing the new Amelia dataset and the Original dataset", fig.pos = "H"}
p_amelia <- df_amelia %>% 
  rownames_to_column(var = "state") %>% 
  pivot_longer(cols = -state, names_to = "year", values_to = "gini") %>% 
  ggplot(aes(x = year, y = gini)) +
  geom_boxplot() + 
  scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
  theme(axis.text.x = element_text(size = 7)) +
  labs(x = "", title = "Amelia dataset")

gridExtra::grid.arrange(p_amelia,p_og, nrow=1)
```

Amelia method somehow deal with the outliers as well, in the year 1918 are reduced to 1, but from 1928 to 1948 there were more new outliers.

```{r}
# mean
df_mean <- data.frame(apply(df_tidy, 2, function(x){
    x[is.na(x)] = mean(x, na.rm = T); x}))
```

The third method is to use the mean to fill in the missing values. Easy computing is the advantage, but it has limitation which could lead to biased estimates, and it doesn't account for uncertainty of the imputed values.

In summary, all 3 methods to some extend handle missing values. While the mean method could create biased estimates, the Amelia method has uncertainty to the imputed values. Hence, we use the mice algorithm to handle the missing value and will use the mice filled-in datasets for the following analysis.

\clearpage

# Principle Component Analysis

Principal components analysis finds a small number of linear combinations of the original variables that explain a large proportion of overall variation in the data. Since the variables under investigation are measured in the same units, we don't need to standardise the data. 

```{r, tidy = TRUE}
completeData %>% 
  prcomp() -> pcaout
summary(pcaout)$importance %>%
  kable(caption = "Table summary for the importance of each principle components") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```

11 principal components are obtained. Each of these explains a percentage of the total variation. That is to say: PC1 explains 58% of the total variance, while PC2 explains 15% of the total variance, as just PC1 and PC2 can explain 73% of the total variance.

```{r, fig.cap = "The plot show the loding score of the first principle components", fig.pos = "H"}
# loadings
as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC1)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC1), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC1") 
```

```{r, fig.cap = "The plot show the loding score of the second principle components", fig.pos = "H"}
as_tibble(pcaout$rotation) %>% 
  mutate(indx = 1:nrow(pcaout$rotation), 
         ymin= rep(0, nrow(pcaout$rotation))) %>% 
  ggplot() + 
    geom_point(aes(x=rownames(pcaout$rotation), y=PC2)) +
    geom_errorbar(aes(x=rownames(pcaout$rotation),
                      ymin=ymin, ymax=PC2), width=0) + 
    geom_hline(yintercept=
                 c(-1/sqrt(nrow(pcaout$rotation)),
                   1/sqrt(nrow(pcaout$rotation))),
               colour="red") +
    scale_x_discrete(labels = as.character(seq(1918, 2018, 10))) +
    theme(axis.text.x = element_text(angle = 90, 
                                     vjust = 0.5, 
                                     hjust=1)) +
  xlab("Predictors") + ylab("PC2") 
```

By visualizing the loading plots, we could identify the variable importance to each principal components. While the red lines represents the confidence interval, variables that exceed the red lines are significantly different from zero, which means having more contribution to the principal components. Year 1918,1928 and 1938 are more influential to PC1, while year 1988,1998,2008 and 2018 are more influential to PC2.

```{r, fig.cap = "The scree plot showing the captured variance for each component"}
# number of PCs
screeplot(pcaout,type = 'l')
```

\clearpage

A scree plot is a diagnostic tool to check whether PCA works well.Here PC1 and PC2 have captured most of the variation. The proportion explained by PCs are significantly decreased after PC2, so selecting 2 PC is reasonable.

```{r, fig.width=8, fig.height=8, fig.cap = "Biplot based on principal components showing the correlation among variables"}
# correlation
biplot(pcaout,scale=0, cex=0.7)
```

By selecting PCs, we visualize the data using biplots. The first biplot shows the correlation among variables. Year 1928, 1918 and 1938 are positively correlated. One possible explanation could be due to the World War 1 and 2 during that period of time. The rest of the variables are positively correlated.

\clearpage

```{r, fig.width=8, fig.height=8, fig.cap = "Biplot based on principal components showing the distance between states"}
# distance
biplot(pcaout, cex=0.7)
```

The second biplot implies the distance. While 1928 contributes the most to the PC1, 2018 contributes the most to the PC2. Also, the closer distance between states, the more similar they are. While most observations are clustered in the top right side, Delaware, New York and Florida are obviously different from the others. The reason for this could be the minimum wage in Delaware is low, while corporations are paying lesser taxes. While some research indicates that wage inequality tends to happen when the demand for skill is the highest, which means that people with higher skills are more in demand, while the need for lower-and-middle-skilled workers declines.


# Multidimensional Scaling

```{r, fig.cap = "The plot showing the low dimensional representation of the dataset using the classical multidimensional scaling", fig.pos = "H"}
dd <- completeData %>%
  dist(method = "euclidean")

completeData %>%
  rownames_to_column() %>%
  pull(rowname) -> attributes(dd)$Labels

cmds <- cmdscale(dd, eig = T)

cmds_point <- cmds$points

cmds_point %>%
  as_tibble(rownames = "State") %>%
  ggplot(aes(x = V1, y = V2, label = State)) +
  geom_text(size = 3) +
  gghighlight(State %in% c("Delaware", "New York", "Florida"), unhighlighted_colour = "dark grey")
```

Using the classical Multidimensional Scaling, it finds a low dimensional representation of the dataset by minimizing strain. From the plot above, we can see the potential three outliers of the dataset; Delaware, New York, and Florida. This result is the same as what we have observed in the PCA. Also, the states that are closer together, the closer they are, as this plot represents the similarity between states.

```{r}
data.frame(cmds$GOF, row.names = c("GF1", "GF2")) %>%
  kable(caption = "The table summary of the goodness of fit from the classical multidimensional scaling") %>%
  kable_styling(latex_options = "hold_position")
```

The *GOF* tells us how good the fit is. The numbers are the same for both measures. It is because we use Euclidean distances, which will always give positive eigenvalues, and since the first measure use $|\lambda_i|$ and the second measure uses $max(0, \lambda_i)$. Therefore when using the Euclidean distance, it will give the same results. With the goodness of fit measures being quite high, we can believe that the solution is an accurate representation.


# Cluster Analysis


```{r, fig.cap = "The dendogram showing the cluster solution using Euclidean distance", fig.pos = "H"}
# df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
#   filter(`1968` != 1000) %>% 
#   column_to_rownames(var = "State") 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
completeData %>% 
    dist(method = 'euclidean')->d #compute distance matrix 
hclust(d,method='ward.D2')->hcl
hcl %>%  plot(cex = 0.6)
hcl %>% rect.hclust(k = 4)
```

From the dendogram above we can see that using the Ward clustering method, we can cut the tree into 4 clusters for stable solutions. Because 3 clusters is not stable and can change over a short range of tolerance, while 5 and above will not include the cluster with Delaware state. 1 and 2 clusters not a good choice as there are room to form better defined clusters.

What's more interesting to see is that, aside from Delaware, we can identify another less smaller outlier which is Alaska. If we go back to the box plot and identify the second most outlying point in Gini index, which is in the year 1988, we can see that the state is indeed Alaska as below

```{r}
boxplot.stats(df$`1988`)$out -> outliers88
df[which(df$`1988` %in% c(outliers88)),] %>%
  kable(caption = "The summary of the potential outlier Alaska") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```

If we use manhattan distance instead to cluster the states, the stable solution would also be 4, similar to that as in the case of euclidean distance. 

```{r, fig.cap = "The dendogram showing the cluster solution using Manhattan distance", fig.pos = "H"}
# df %>% #pivot_longer(cols = -State, names_to = "year", values_to = "gini") %>% 
#   filter(`1968` != 1000) %>% 
#   column_to_rownames(var = "State") %>% 
  # pivot_wider(names_from = year,
  #             values_from = gini) %>% 
completeData %>% 
    dist(method = 'manhattan')->dman #compute distance matrix 
hclust(dman,method='ward.D2')->hclman
hclman %>%  plot(cex = 0.6)
hclman %>% rect.hclust(k = 4)
```


To see if the members of the state are different or not as compared to when euclidean distance is used, we can assume the cluster is 4 and use Rand index:

```{r}
library(mclust) 
hclman %>% cutree(k = 4) -> memb_man
hcl %>% cutree(k = 4) -> memb_eu

rand <- data.frame(adjustedRandIndex(memb_eu,memb_man))
colnames(rand) <- "Rand index"

rand %>%
  kable(caption = "Table showing the adjusted rand index") %>%
  kable_styling(latex_options = "hold_position")
```

It's interesting to see how when different distance is used, the number of clusters are still the same but the members of each cluster are all difference, as the adjusted Rand Index is only 0.2

# Limitation

- **Data:** the obvious limitation of this data is the null values, although using many different ways to compare and finally choosing the Predictive Mean Matching from the MICE package to reassign, it is also small biased from the actual situation.
- **PCA:** one limitation is the low interpretability of PCA, although the first two PCs have explained 73% of the total variance, it is difficult to generalize what the new index specifically means. On the other hand, PCA is sensitive to the scale of the features and not robust against outliers, so we tried a variety of NA reassignment methods to minimize the bias from the data.


